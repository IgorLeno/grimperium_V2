"""Quality threshold monitoring for CREST-PM7 Pipeline.

Implements 5 detection patterns for monitoring pipeline health
and generating alerts.
"""

import logging
from collections import deque
from dataclasses import dataclass, field
from datetime import datetime
from typing import Callable, Optional

from .config import AlertLevel, PM7Config, QualityGrade

LOG = logging.getLogger("grimperium.crest_pm7.threshold_monitor")


@dataclass
class Alert:
    """Alert generated by threshold monitoring.

    Attributes:
        level: Alert severity level
        pattern: Detection pattern that triggered the alert
        message: Human-readable alert message
        timestamp: When the alert was generated
        metrics: Relevant metrics at time of alert
    """

    level: AlertLevel
    pattern: str
    message: str
    timestamp: datetime = field(default_factory=datetime.now)
    metrics: dict = field(default_factory=dict)


@dataclass
class MonitoringMetrics:
    """Current monitoring metrics.

    Attributes:
        total_processed: Total molecules processed
        success_count: Number of successful molecules
        failure_count: Number of failed molecules
        hof_extraction_success: HOF extraction successes
        hof_extraction_failure: HOF extraction failures
        grade_a_count: Grade A results
        grade_b_count: Grade B results
        grade_c_count: Grade C results
        grade_failed_count: Grade FAILED results
        timeout_count: Timeout failures
        scf_failure_count: SCF convergence failures
        consecutive_failures: Current consecutive failure streak
    """

    total_processed: int = 0
    success_count: int = 0
    failure_count: int = 0
    hof_extraction_success: int = 0
    hof_extraction_failure: int = 0
    grade_a_count: int = 0
    grade_b_count: int = 0
    grade_c_count: int = 0
    grade_failed_count: int = 0
    timeout_count: int = 0
    scf_failure_count: int = 0
    consecutive_failures: int = 0

    @property
    def success_rate(self) -> Optional[float]:
        """Overall success rate.

        Returns:
            Success rate as float, or None if no samples processed.
        """
        if self.total_processed == 0:
            return None
        return self.success_count / self.total_processed

    @property
    def hof_extraction_rate(self) -> Optional[float]:
        """HOF extraction success rate.

        Returns:
            Extraction rate as float, or None if no samples.
        """
        total = self.hof_extraction_success + self.hof_extraction_failure
        if total == 0:
            return None
        return self.hof_extraction_success / total

    @property
    def grade_ab_rate(self) -> Optional[float]:
        """Grade A+B rate.

        Returns:
            A+B rate as float, or None if no samples.
        """
        if self.total_processed == 0:
            return None
        return (self.grade_a_count + self.grade_b_count) / self.total_processed


class ThresholdMonitor:
    """Monitors pipeline quality metrics and generates alerts.

    Implements 5 detection patterns:
    1. Success rate drop
    2. HOF extraction failure spike
    3. Consecutive failures
    4. Grade degradation
    5. Timeout/SCF pattern detection
    """

    def __init__(self, config: PM7Config) -> None:
        """Initialize monitor.

        Args:
            config: Pipeline configuration
        """
        self.config = config
        self.metrics = MonitoringMetrics()
        self.alerts: list[Alert] = []

        # Rolling window for recent results
        self.window_size = config.monitor_window_size
        self.recent_successes: deque[bool] = deque(maxlen=self.window_size)
        self.recent_hof_extractions: deque[bool] = deque(maxlen=self.window_size)
        self.recent_grades: deque[QualityGrade] = deque(maxlen=self.window_size)

        # Alert callbacks
        self._alert_callbacks: list[Callable[[Alert], None]] = []

    def register_callback(self, callback: Callable[[Alert], None]) -> None:
        """Register a callback for alerts.

        Args:
            callback: Function to call when alert is generated
        """
        self._alert_callbacks.append(callback)

    def _emit_alert(self, alert: Alert) -> None:
        """Emit an alert to all registered callbacks."""
        self.alerts.append(alert)
        LOG.log(
            logging.CRITICAL if alert.level == AlertLevel.CRITICAL else
            logging.WARNING if alert.level == AlertLevel.WARNING else
            logging.INFO,
            f"[{alert.level.value}] {alert.pattern}: {alert.message}"
        )
        for callback in self._alert_callbacks:
            try:
                callback(alert)
            except Exception as e:
                LOG.error(f"Alert callback failed: {e}")

    def _is_critical_condition(self) -> bool:
        """Check if current state is critical.

        Shared helper to avoid duplicated threshold logic.

        Returns:
            True if critical conditions are met
        """
        # Check consecutive failures
        if self.metrics.consecutive_failures >= self.config.consecutive_failures_critical:
            return True

        # Check success rate
        half_window = max(1, self.window_size // 2)
        if len(self.recent_successes) >= half_window:
            window_rate = sum(self.recent_successes[-half_window:]) / half_window
            if window_rate < self.config.success_rate_critical:
                return True

        return False

    def record_result(
        self,
        success: bool,
        grade: QualityGrade,
        hof_extracted: bool,
        timeout: bool = False,
        scf_failed: bool = False,
    ) -> list[Alert]:
        """Record a molecule processing result.

        Args:
            success: Whether processing succeeded
            grade: Quality grade assigned
            hof_extracted: Whether HOF was successfully extracted
            timeout: Whether timeout occurred
            scf_failed: Whether SCF convergence failed

        Returns:
            List of alerts generated from this result
        """
        new_alerts: list[Alert] = []

        # Update metrics
        self.metrics.total_processed += 1

        if success:
            self.metrics.success_count += 1
            self.metrics.consecutive_failures = 0
        else:
            self.metrics.failure_count += 1
            self.metrics.consecutive_failures += 1

        if hof_extracted:
            self.metrics.hof_extraction_success += 1
        else:
            self.metrics.hof_extraction_failure += 1

        # Update grade counts
        if grade == QualityGrade.A:
            self.metrics.grade_a_count += 1
        elif grade == QualityGrade.B:
            self.metrics.grade_b_count += 1
        elif grade == QualityGrade.C:
            self.metrics.grade_c_count += 1
        else:
            self.metrics.grade_failed_count += 1

        if timeout:
            self.metrics.timeout_count += 1
        if scf_failed:
            self.metrics.scf_failure_count += 1

        # Update rolling windows
        self.recent_successes.append(success)
        self.recent_hof_extractions.append(hof_extracted)
        self.recent_grades.append(grade)

        # Run detection patterns
        new_alerts.extend(self._check_success_rate_drop())
        new_alerts.extend(self._check_hof_extraction_failure())
        new_alerts.extend(self._check_consecutive_failures())
        new_alerts.extend(self._check_grade_degradation())
        new_alerts.extend(self._check_timeout_scf_pattern())

        return new_alerts

    def _check_success_rate_drop(self) -> list[Alert]:
        """Pattern 1: Check for success rate drop."""
        alerts = []

        # Only check if we have enough data
        if len(self.recent_successes) < self.window_size // 2:
            return alerts

        window_rate = sum(self.recent_successes) / len(self.recent_successes)

        if window_rate < self.config.success_rate_critical:
            alerts.append(Alert(
                level=AlertLevel.CRITICAL,
                pattern="success_rate_drop",
                message=f"Success rate critically low: {window_rate:.1%} < {self.config.success_rate_critical:.1%}",
                metrics={
                    "window_rate": window_rate,
                    "threshold": self.config.success_rate_critical,
                    "window_size": len(self.recent_successes),
                },
            ))
            self._emit_alert(alerts[-1])

        elif window_rate < self.config.success_rate_warning:
            alerts.append(Alert(
                level=AlertLevel.WARNING,
                pattern="success_rate_drop",
                message=f"Success rate below warning: {window_rate:.1%} < {self.config.success_rate_warning:.1%}",
                metrics={
                    "window_rate": window_rate,
                    "threshold": self.config.success_rate_warning,
                },
            ))
            self._emit_alert(alerts[-1])

        return alerts

    def _check_hof_extraction_failure(self) -> list[Alert]:
        """Pattern 2: Check for HOF extraction failure spike."""
        alerts = []

        # Use threshold from config
        if len(self.recent_hof_extractions) < self.config.hof_extraction_min_samples:
            return alerts

        recent_rate = sum(self.recent_hof_extractions) / len(self.recent_hof_extractions)

        if recent_rate < self.config.hof_extraction_threshold:
            alerts.append(Alert(
                level=AlertLevel.WARNING,
                pattern="hof_extraction_failure",
                message=f"HOF extraction rate low: {recent_rate:.1%}",
                metrics={
                    "extraction_rate": recent_rate,
                    "threshold": self.config.hof_extraction_threshold,
                },
            ))
            self._emit_alert(alerts[-1])

        return alerts

    def _check_consecutive_failures(self) -> list[Alert]:
        """Pattern 3: Check for consecutive failures."""
        alerts = []

        if self.metrics.consecutive_failures >= self.config.consecutive_failures_critical:
            alerts.append(Alert(
                level=AlertLevel.CRITICAL,
                pattern="consecutive_failures",
                message=f"{self.metrics.consecutive_failures} consecutive failures",
                metrics={"consecutive_failures": self.metrics.consecutive_failures},
            ))
            self._emit_alert(alerts[-1])

        elif self.metrics.consecutive_failures >= self.config.consecutive_failures_warning:
            alerts.append(Alert(
                level=AlertLevel.WARNING,
                pattern="consecutive_failures",
                message=f"{self.metrics.consecutive_failures} consecutive failures",
                metrics={"consecutive_failures": self.metrics.consecutive_failures},
            ))
            self._emit_alert(alerts[-1])

        return alerts

    def _check_grade_degradation(self) -> list[Alert]:
        """Pattern 4: Check for grade quality degradation."""
        alerts = []

        if len(self.recent_grades) < self.config.grade_degradation_min_samples:
            return alerts

        ab_count = sum(1 for g in self.recent_grades if g in (QualityGrade.A, QualityGrade.B))
        ab_rate = ab_count / len(self.recent_grades)

        if ab_rate < self.config.grade_degradation_threshold:
            alerts.append(Alert(
                level=AlertLevel.WARNING,
                pattern="grade_degradation",
                message=f"Quality grade degradation: {ab_rate:.1%} A/B grades",
                metrics={
                    "ab_rate": ab_rate,
                    "threshold": self.config.grade_degradation_threshold,
                },
            ))
            self._emit_alert(alerts[-1])

        return alerts

    def _check_timeout_scf_pattern(self) -> list[Alert]:
        """Pattern 5: Check for timeout/SCF failure pattern."""
        alerts = []

        if self.metrics.total_processed < 10:
            return alerts

        timeout_rate = self.metrics.timeout_count / self.metrics.total_processed
        scf_rate = self.metrics.scf_failure_count / self.metrics.total_processed

        if timeout_rate > self.config.timeout_pattern_threshold:
            alerts.append(Alert(
                level=AlertLevel.WARNING,
                pattern="timeout_pattern",
                message=f"High timeout rate: {timeout_rate:.1%}",
                metrics={
                    "timeout_rate": timeout_rate,
                    "timeout_count": self.metrics.timeout_count,
                },
            ))
            self._emit_alert(alerts[-1])

        if scf_rate > self.config.scf_pattern_threshold:
            alerts.append(Alert(
                level=AlertLevel.WARNING,
                pattern="scf_failure_pattern",
                message=f"High SCF failure rate: {scf_rate:.1%}",
                metrics={
                    "scf_rate": scf_rate,
                    "scf_count": self.metrics.scf_failure_count,
                },
            ))
            self._emit_alert(alerts[-1])

        return alerts

    def get_summary(self) -> dict:
        """Get monitoring summary.

        Returns:
            Dictionary with all metrics and alert counts
        """
        return {
            "total_processed": self.metrics.total_processed,
            "success_rate": self.metrics.success_rate,
            "hof_extraction_rate": self.metrics.hof_extraction_rate,
            "grade_ab_rate": self.metrics.grade_ab_rate,
            "grade_distribution": {
                "A": self.metrics.grade_a_count,
                "B": self.metrics.grade_b_count,
                "C": self.metrics.grade_c_count,
                "FAILED": self.metrics.grade_failed_count,
            },
            "timeout_count": self.metrics.timeout_count,
            "scf_failure_count": self.metrics.scf_failure_count,
            "alert_count": len(self.alerts),
            "critical_alerts": sum(1 for a in self.alerts if a.level == AlertLevel.CRITICAL),
            "warning_alerts": sum(1 for a in self.alerts if a.level == AlertLevel.WARNING),
        }

    def should_pause(self) -> bool:
        """Determine if pipeline should pause for review.

        Uses shared helper to check critical conditions.

        Returns:
            True if critical conditions detected
        """
        # Use helper to check critical conditions
        if self._is_critical_condition():
            return True

        # Also check if there are recent critical alerts
        recent_alerts = self.alerts[-10:] if len(self.alerts) > 10 else self.alerts
        if any(a.level == AlertLevel.CRITICAL for a in recent_alerts):
            return True

        return False
